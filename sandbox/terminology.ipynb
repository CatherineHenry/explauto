{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de63afb8-b239-4280-ab6e-cd8364e049a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30471d1-d996-4633-b855-9d9b854fed2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Environment\n",
    "Implements the physical properties between the robot body and the environment in which it evolves (can be real or simulated) \n",
    "    - Each environment implements its own `compute_sensori_effect` function which takes motor command vector $m$ and returns the corresponding sensory effect vector $s$\n",
    "    - An environment config  provides information about the motor and sensori ranges used by the environment \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427dddbc-4cb7-4a3a-b7f2-cfa971195920",
   "metadata": {},
   "source": [
    "## Sensorimotor Model\n",
    "Learns mappings between robot motor actions and the sensory effect they produce. Implements both:\n",
    "    1. The **iterative learning process** from sensorimotor experience, i.e. from the iterative collection of  (ùëö,ùë†) pairs by interaction with the environment\n",
    "    2. The use of the resulting internal model to perform **forward and inverse predictions** (or any kind of general prediction between sensorimotor subspaces).\n",
    "\n",
    "As a result, every Sensorimotor Model will be able to: \n",
    "- Infer the \\<sensory result\\> from a given motor command (forward prediciton)\n",
    "- Infer the motor command allowing to reach a particular \\<sensory result\\> (inverse prediction)\n",
    "- Update online from sensorimotor experience\n",
    "\n",
    "\n",
    "Explauto implements various sensorimotor models: Simple nearest-neighbor look-up, non-parametric models combining classical regressions and optimization algorithms, online local mixtures of Gaussians (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aec331-f4d1-4423-9965-04d0a2279e64",
   "metadata": {},
   "source": [
    "    \n",
    "## Interest Space\n",
    "Areas of exploration from which goals are sampled. The interest space can be either;\n",
    "- Motor space: Results in **motor babbling** strategies\n",
    "- Sensory space: Results in **goal babbling** strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101d09d-ab18-4b9f-80f1-76f2edb5662c",
   "metadata": {},
   "source": [
    "## Interest Model\n",
    "Implements the active exploration process (curiosity). Explores the interest space (either motor or sensory) by sampling goals using a [**sampling procedure**](#Sampling-Procedures). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90582cec-2ddf-4c6e-a51a-01988572e1fc",
   "metadata": {},
   "source": [
    "## Agent\n",
    "Encapsulates sensorimotor and interest models\n",
    "- Allows to generalize and simplify the simulation loop\n",
    "- Removes bootstrapping issues encountered when training a sensorimotor model while also needing initial inverse prediction(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a709e2c-744e-42e7-8a64-879be62d71ac",
   "metadata": {},
   "source": [
    "## High-Level Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84877515-7d0a-4502-be44-a430eb8b49d9",
   "metadata": {},
   "source": [
    "An example of motor babbling implementation without using an Agent (to clearly show the steps): \n",
    "\n",
    "```python\n",
    "# Instantiate an interest model\n",
    "im_model = InterestModel.from_configuration(environment_conf=, interest_space_dimensions=, interest_model_name=, config_name=)\n",
    "for _ in range(100):\n",
    "    # sample a sensory goal maximizing learning progress using the interest model:\n",
    "    motor_goal = im_model.sample()\n",
    "    # use a trained sensorimodel to make a forward prediction from the motor goal to expected sensory effect:\n",
    "    # Note: Agent would handle initial bootstrapping on sensorimodel\n",
    "    sensori_inferred = sm_model.forward_prediction(motor_goal)\n",
    "    # execute the command and observe the corresponding sensory effect:\n",
    "    sensori_actual = environment.compute_sensori_effect(motor_goal)\n",
    "    # update the sensorimodel:\n",
    "    sm_model.update(motor_goal, sensori_actual)\n",
    "    # update the interestmodel:\n",
    "    im_model.update(np.hstack((motor_goal, sensori_actual)), np.hstack((motor_goal, sensori_inferred)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513cff5d-598f-4704-8329-18008dff8bcd",
   "metadata": {},
   "source": [
    "# Sensorimotor Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf2c08-c7a5-4553-8ae1-3b6864f24465",
   "metadata": {},
   "source": [
    "- **Forward Models**: Learn relationship of motor controllers to sensory effects\n",
    "- **Inverse Models**: Learn relationship of sensory effects (goals) to the motor program/action allowing to reach them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a3ae1-222c-467c-9a68-b6a026b1f62d",
   "metadata": {},
   "source": [
    "* `xy` -> motor order/sensory effect pair to the model\n",
    "    - :arg x:  an input (order) vector compatible with self.Mfeats.\n",
    "    - :arg y:  a output (effect) vector compatible with self.Sfeats.\n",
    "    \n",
    "* `m`: Motor part of `x,y`\n",
    "* `s`: Sensory part of `x,y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151b6a14-3eb0-4140-9f15-6b097b2a4b33",
   "metadata": {},
   "source": [
    "## Online Learning Algorithms (Models)\n",
    "Trained iteratively during the interaction of the robot in the environment in which it evolves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c23d64-5d99-4d37-8c66-9cc46ce8722f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Non-Parametric \n",
    "Combining classical regressions and optimization algorithms\n",
    "\n",
    "Consists of the following:\n",
    "- **dataset**: Stores all the experiments ($m$, $s$) into a list\n",
    "- **forward model**: Uses the dataset for the forward prediction\n",
    "- **inverse model**: Uses the forward model _or_ the dataset directly to perform inverse prediction\n",
    "\n",
    "Two operating modes:\n",
    "- explore: When the agent asks for the exact inverse prediction $m$ of a goal $s_g$, $m$ will be perurbated with some gaussian exploration noise in order to allow the agent to explore new motor commands.\n",
    "    - As a result, the sensormitor models have a common parameter `sigma_explo_ratio` (default = 0.1) which is the standard deviation of the gausssian noise scaled depending on the motor domain size. If a motor value is bounded in [-2:2], then a `sigma_explo_ratio` of 0.1 will induce an exploration noise of (`m_max` - `m_min`) * `sigma_explo_ratio`). 0.4 in this example. \n",
    "- exploit: No exploration noise is added. This mode is used for instance when evaluating the inverse model for comparison purposes.\n",
    "\n",
    "\n",
    "Can use combinations of forward and inverse models -- there are some provided or can define your own. For example, `LWLR-CMAES` will use a forward model of `LWLR` and an inverse model `CMAES`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b1795b-90f5-46c2-93fe-c46fa78d67f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Forward Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a069b69-4a92-4348-8702-e64d9fd9c025",
   "metadata": {},
   "source": [
    "Predict $s_p$ given a $m$ that might have never been observed, using the dataset of observations ($m$, $s$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f4c5e-bad7-4f5f-81b7-4c6eb096d9ab",
   "metadata": {},
   "source": [
    "##### NN (Nearest Neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a62c00-b826-459c-b6b0-f7fa871a3158",
   "metadata": {},
   "source": [
    "Works sufficiently well in different typical robotic applications. \n",
    "\n",
    "To perform a forward prediction the Nearest Neighbor model looks in the dataset of tuples ($m$, $s$) for the nearest neighbor of the given $m$ motor command, and returns it's corresponding $s$. This forward model is very fast (up to datasets of size $10^5$) and makes no assumptions about hte regularity of the model being learned (continuity, linearity, ...). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e6949-ad6f-44c1-be9a-4efd3666fe4e",
   "metadata": {},
   "source": [
    "##### NSNN (Non-Stationary Nearest Neighbor)\n",
    "Modified version of NN for non-stationary environments.\n",
    "\n",
    "Points are not only weighted by distance but also by the number of points that appeared after that one (gaussian with parameter `sigma_t`=100), to put less weight on old points and allow the learning of non-stationary environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691d215-7c18-4a22-b217-1c03d1728f7e",
   "metadata": {},
   "source": [
    "##### WNN (Weighted Nearest Neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5ce72-67d1-49d6-80a0-db0d819b7489",
   "metadata": {},
   "source": [
    "To perform a forward prediction of $m$, the Weighted Nearest Neighbor model looks at the $k$ nearest neighbors of $m$ in the dataset and returns the average of the $k$ corresponding $s$. This average is weighted by the distance to $m$ with a gaussian of standard deviation $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ec157-79da-4506-8f9e-7c2a143684f6",
   "metadata": {},
   "source": [
    "##### ES-WNN\n",
    "\n",
    "WNN with estimated sigma, on a query basis, as the mean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f5769-ab5e-4d90-8001-cfc82f983e62",
   "metadata": {},
   "source": [
    "##### LWLR (Locally Weighted Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c610e-bdae-4403-9f50-15e7443f76fb",
   "metadata": {},
   "source": [
    "Computes a linear regression of the $k$ nearest neighbors of $m$ (thus a local regression) and finds the requested $s$ with the given $m$ based on that regression. \n",
    "\n",
    "References :\n",
    "1. https://en.wikipedia.org/wiki/Local_regression\n",
    "2. C. G. Atkeson, A. W. Moore, S. Schaal, \"[Locally Weighted Learning for Control](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.468.7121&rep=rep1&type=pdf)\", \"Springer Netherlands\", 75-117, vol 11, issue 1, 1997/02, 10.1023/A:1006511328852    \n",
    "3. See also a [video](http://www.cosmolearning.com/video-lectures/locally-weighted-regression-probabilistic-interpretation-logistic-regression/) lecture on LWR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ccbb35-25f4-4931-a06f-ac9faec2eb80",
   "metadata": {
    "tags": []
   },
   "source": [
    "Pseudo Code: \n",
    "```python\n",
    "Input D problem dimension\n",
    "Input X matrix of inputs:  X[k][i] = i‚Äôth component of k‚Äôth input point.\n",
    "Input Y matrix of outputs: Y[k] = k‚Äôth output value.\n",
    "Input xq = query input.    Input kwidth.\n",
    "\n",
    "WXTWX = empty (D+1) x (D+1) matrix\n",
    "WXTWY = empty (D+1) x 1     matrix\n",
    "\n",
    "for ( k = 0 ; i <= N - 1 ; i = i + 1 )\n",
    "    # Compute weight of kth point\n",
    "    wk = weight_function( distance( xq , X[k] ) / kwidth )\n",
    "\n",
    "    /* Add to (WX) ^T (WX) matrix */\n",
    "    for ( i = 0 ; i <= D ; i = i + 1 )\n",
    "        for ( j = 0 ; j <= D ; j = j + 1 )\n",
    "            if ( i == 0 )\n",
    "                xki = 1 else xki = X[k] [i]\n",
    "            if ( j == 0 )\n",
    "                xkj = 1 else xkj = X[k] [j]\n",
    "            WXTWX [i] [j] = WXTWX [i] [j] + wk * wk * xki * xkj\n",
    "\n",
    "    /*  Add to (WX) ^T (WY) vector */\n",
    "    for ( i = 0 ; i <= D ; i = i + 1 )\n",
    "        if ( i == 0 )\n",
    "            xki = 1 else xki = X[k] [i]\n",
    "        WXTWY [i] = WXTWY [i] + wk * wk * xki * Y[k]\n",
    "\n",
    "/* Compute the local beta.  Call your favorite linear equation solver.\n",
    "   Recommend Cholesky Decomposition for speed.\n",
    "   Recommend Singular Val Decomp for Robustness. */\n",
    "\n",
    "Beta = (WXTWX)^{-1}(WXTWY)\n",
    "\n",
    "Output ypredict = beta[0] + beta[1]*xq[1] + beta[2]*xq[2] + ‚Ä¶ beta[D]*x q[D]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2bcd54-7220-48fc-ad4a-f696b8ac1d70",
   "metadata": {},
   "source": [
    "##### NSLWLR (Non-Stationary Locally Weighted Linear Regression)\n",
    "Modified version of LWLR for non-stationary environments.\n",
    "\n",
    "Points are not only weighted by distance but also by the number of points that appeared after that one (gaussian with parameter `sigma_t`=100), to put less weight on old points and allow the learning of non-stationary environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9bebd-08b9-4803-abab-ba23c6adcc5d",
   "metadata": {},
   "source": [
    "##### ES-LWLR\n",
    "\n",
    "LWLR with estimated sigma, on a query basis, as the mean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424ffec-c8fe-4a00-9df2-caa8bd8003a1",
   "metadata": {},
   "source": [
    "#### Inverse Models\n",
    "\n",
    "Inverse models infer a motor command $m$ that should be able to reach a given goal $s_g$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd1a9d-dfe5-4c21-b85b-b410b04c1ba8",
   "metadata": {},
   "source": [
    "##### NN (Nearest Neighbor)\n",
    "\n",
    "To perform the inverse inference, the Nearest Neighbor inverse model looks in the dataset of tuples $(m, s)$ for the nearest neighbor of the given $s$ motor command, and return its corresponding $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54942150-117c-4ab4-9082-7e4aec56997b",
   "metadata": {},
   "source": [
    "##### NSNN (Non-Stationary Nearest Neighbor)\n",
    "Modified version of NN for non-stationary environments.\n",
    "\n",
    "Points are not only weighted by distance but also by the number of points that appeared after that one (gaussian with parameter `sigma_t`=100), to put less weight on old points and allow the learning of non-stationary environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca5c67-d477-435b-89c1-1adfc988b8ef",
   "metadata": {},
   "source": [
    "##### WNN (Weighted Nearest Neighbor)\n",
    "\n",
    "Typical robotic forward models are very redundant. For example, a robotic arm can move it's hand to position $s$ with infinite possible $m$ motor positions.\n",
    "\n",
    "As a result, when trying to infer a motor command $m$ to reach a given goal $s$ an average of the nearest neighbors of $s$ in the dataset would make no sense as those nearest neighbors might have very different corresponding motor commands.\n",
    "\n",
    "To perform the inverse inference of a given $s$, the Weighted Nearest Neighbor model looks at the nearest neighbor of $s$ in the dataset and gets its corresponding $m$. It then finds the $k$ (parameter) nearest neighbors of $m$ in the dataset, and returns their average weighted by the distance of their sensory part to $s$, with a gaussian of standard deviation $\\sigma$ (parameter).\n",
    "\n",
    "See code [here](https://github.com/flowersteam/explauto/blob/master/explauto/sensorimotor_model/inverse/wnn.py#L25)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067851e-a0ba-46e7-9ac9-61a4b040a50b",
   "metadata": {},
   "source": [
    "##### ES-WNN\n",
    "\n",
    "WNN with estimated sigma, on a query basis, as the mean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8ce19-5b15-44df-9779-827dc17ee789",
   "metadata": {},
   "source": [
    "##### Jacobian\n",
    "\n",
    "TODO: find more details on their implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aee774-2a82-4df2-b11a-ed2c4c0cbb6a",
   "metadata": {},
   "source": [
    "##### COBYLA (Constrained Optimization BY Linear Approximation) \n",
    "\n",
    "**Optimization algorithm** to minimize the error $e(x) = ||f(x) - y_g||^2$  where $y_g$ is the goal, $f$ is the forward model, and $x$ is the motor command to be inferred.\n",
    "\n",
    "A numerical optimization method for constrained problems where the derivative of the objective function is not known\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cobyla.html\n",
    "\n",
    "COBYLA specific param:\n",
    "* `maxiter`: Limits the number of error function (and as a result, forward model) evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c88aac5-3858-469d-8869-70f87d995c12",
   "metadata": {},
   "source": [
    "##### BFGS (Broyden-Fletcher-Goldfarb-Shanno Optimization Algorithm)\n",
    "\n",
    "_**Optimization algorithm** to minimize the error $e(x) = ||f(x) - y_g||^2$  where $y_g$ is the goal, $f$ is the forward model, and $x$ is the motor command to be inferred._\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html\n",
    "\n",
    "BFGS specific param:\n",
    "* `maxfun`: Limits the number of error function (and as a result, forward model) evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e963286-3d38-4a33-b218-7fd5c4f55697",
   "metadata": {},
   "source": [
    "##### L-BFGS-B (Limited Memory Bounded Broyden-Fletcher-Goldfarb-Shanno Optimization Algorithm)\n",
    "\n",
    "**Optimization algorithm** to minimize the error $e(x) = ||f(x) - y_g||^2$  where $y_g$ is the goal, $f$ is the forward model, and $x$ is the motor command to be inferred.\n",
    "\n",
    "A limited-memory quasi-Newton code for bound-constrained optimization\n",
    "\n",
    "https://en.wikipedia.org/wiki/Limited-memory_BFGS  \n",
    "http://users.iems.northwestern.edu/~nocedal/lbfgsb.html  \n",
    "https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html\n",
    "\n",
    "BFGS specific param:\n",
    "* `maxfun`: Limits the number of error function (and as a result, forward model) evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da94f45-daf7-435d-b1ec-afc11851d005",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### CMA-ES (Covariance Matrix Adaptation - Evolutionary Strategy)\n",
    "\n",
    "**Optimization algorithm** to minimize the error $e(x) = ||f(x) - y_g||^2$  where $y_g$ is the goal, $f$ is the forward model, and $x$ is the motor command to be inferred.\n",
    "\n",
    "Inverse model also optimizes the error function above but makes fewer assumptions on the regularity of the forward model to perform the search. It is based on a random exploration (with a computed covariance) around a current point of interest, and adapts this point and recompute the covariance matrix at each iteration, with memory of the taken path.\n",
    "\n",
    "The initial point is set as the motor part $m$ of the nearest neighbor $s$ of the goal $s_g$, and the initial covariance matrix is identity times an exploration $\\sigma$ (parameter). This inverse model also takes a 'maxfevals' parameter that limits the number of forward model evaluations.\n",
    "\n",
    "See [Hansen's website](http://www.cmap.polytechnique.fr/~nikolaus.hansen/) and this [tutorial](https://arxiv.org/abs/1604.00772) on CMA-ES."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a629e4c-8522-4885-a739-b1fc35f1fb5a",
   "metadata": {},
   "source": [
    "### IMLE\n",
    "\n",
    "**‚ö†Ô∏è** **Appears unused**\n",
    "\n",
    "IMLE model from Bruno Damas\n",
    "\n",
    "> We present a supervised learning algorithm for estimation of generic input-output relations in a real-time, online fashion. The proposed method is based on a generalized expectation-maximization approach to fit an infinite mixture of linear experts (IMLE) to an online stream of data samples. This probabilistic model, while not fully Bayesian, can efficiently choose the number of experts that are allocated to the mixture, this way effectively controlling the complexity of the resulting model. The result is an incremental, online, and localized learning algorithm that performs nonlinear, multivariate regression on multivariate outputs by approximating the target function by a linear relation within each expert input domain and that can allocate new experts as needed. A distinctive feature of the proposed method is the ability to learn multivalued functions: one-to-many mappings that naturally arise in some robotic and computer vision learning domains, using an approach based on a Bayesian generative model for the predictions provided by each of the mixture experts. As a consequence, it is able to directly provide forward and inverse relations from the same learned mixture model. We conduct an extensive set of experiments to evaluate the proposed algorithm performance, and the results show that it can outperform state-of-the-art online function approximation algorithms in single-valued regression, while demonstrating good estimation capabilities in a multivalued function approximation context.\n",
    "\n",
    "More details:\n",
    "\n",
    "- https://direct.mit.edu/neco/article-abstract/25/11/3044/7931/Online-Learning-of-Single-and-Multivalued\n",
    "- https://github.com/bdamas/IMLE\n",
    "- http://users.isr.ist.utl.pt/~bdamas/IMLE (Note! I have not tried downloading the zip...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d060d83-6f2a-46b9-a398-8bfcc343c6a8",
   "metadata": {},
   "source": [
    "### ~ILO GMM~\n",
    "\n",
    "**‚ö†Ô∏è** **Appears unused**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71fb6f4-ce3e-4c1f-8298-2bb7991f7805",
   "metadata": {},
   "source": [
    "### ~Bayesian Optimisation~\n",
    "\n",
    "**‚ö†Ô∏è** **Appears unused**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aab16c-af82-4e3a-a668-c30dbca3137e",
   "metadata": {},
   "source": [
    "### Discrete (LidstoneModel)\n",
    "\n",
    "**‚ö†Ô∏è** **Appears to have only been used with DiscretizedProgress**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e9bc1-2d29-4596-921d-4b641c0103ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interest Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956c6f7-2ff2-40fe-bbfd-033ef6920a7c",
   "metadata": {},
   "source": [
    "## Competence\n",
    "\n",
    "At each iteration:\n",
    "1. A goal is selected by the interest model \n",
    "2. The sensorimotor model tries to reach that goal \n",
    "3. The **competence** of the goal is computed\n",
    "\n",
    "There are three ways to determine the **competence** of a goal:\n",
    "* **Distance**: The distance between the actual reached point and the goal\n",
    "    - Used by Discretized sampling procedure\n",
    "* **Exponential**:  $e^{-power\\times||g-s||}$\n",
    "    - Used by Tree sampling procedur\n",
    "* **Bool**: Return goal == reached\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec3a47-2ae3-4d54-99d7-981d97bdd0c5",
   "metadata": {},
   "source": [
    "## Sampling Procedures\n",
    "\n",
    "Sample the interest space. For non-random sampling procedures, the sampled goal should serve to improve the prediction of the sensorimotor model by maximizing learning progress. Estimates how a given action is useful for learning and samples the best ones\n",
    "\n",
    "**Sensorimotor Experiment**: The selection of a sample with the purpose of improving the sensorimotor model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcea37-9629-4d57-bb1c-102814079bff",
   "metadata": {},
   "source": [
    "### Random Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d63f5f-1845-49fe-adfc-9063b17b4964",
   "metadata": {},
   "source": [
    "Uniformly sample the choice space ranges // draws random goals in the interest space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de885229-81bf-457f-8915-4393ae8dd43b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Discretized Progress\n",
    "Divides sensorimotor choice space into a grid of dimensions $sensorimotor\\_mins \\times sensorimotor\\_maxs$ and maintains an empirical measure of the learning progress in each cell. Samples a cell according to the **learning progress** (favoring cells displaying high progresses) and samples a random point in that chosen cell. Each cell keeps a history of the recent learning errors observed.\n",
    "\n",
    "**‚ö†Ô∏è** If the number of sensorimotor dimensions is large (>2), the _discretization progress procedure _won't be feasible_ as the number of regions is exponential in the number of dimensions. \n",
    "\n",
    "- $x$: A sample point in the choice space $X$ (`expl_dims`) (i.e. a choice)\n",
    "    - Motor Babbling: $X$ corresponds to the motor space $M$ \n",
    "    - Goal Babbling: $X$ corresponds to sensory space $S$   \n",
    "<br>\n",
    "<br>\n",
    "-  $y$: Prediction performed by the sensorimotor model for the choice $x$ in space $Y$ (`inf_dims`)\n",
    "    - Motor Babbling: A **forward** prediction where $Y=S$\n",
    "    - Goal Babbling: An **inverse** prediction where $Y=M$ \n",
    "<br>\n",
    "<br>\n",
    "- $xy$: Concatenation of $x$ and $y$ reordered as a vector in $M \\times S$\n",
    "\n",
    "- $m$: The executed motor command\n",
    "- $s$: The observed sensory consequence \n",
    "\n",
    "- `x_card`: The total numbers of cells in the discretization\n",
    "- `win_size`: window size of the interest computation which is based on the last `win_size` points\n",
    "\n",
    "- **Learning Error**: The distance between $xy$ and $ms$ \n",
    "- **Learning Progress**: The opposite of covariance between time (relative to a particular cell) and **learning error** (i.e. the agent is progressing in that cell if the covariance between time and error is negative). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f79bd-a30d-4616-9001-9d4279b91eb0",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Tree Progress\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e9f74-e38b-4df4-b6ed-5c333f011e6e",
   "metadata": {},
   "source": [
    "Adapts the discretization to the dataset distribution. At each iteration, if there are too many points in a region, that region is split into two subregions (along the next axis in a kdtree-like way). The value of the split is chosen to best discriminate the interest of the 2 subregions.\n",
    "\n",
    "\n",
    "* `max_points_per_region`:  Maximum number of points per region. A given region is split when this number is exceeded.\n",
    "* `max_depth`: Maximum depth of the tree\n",
    "* `split_mode`: Mode to split a region: \n",
    "    - `random`: random value between first and last points\n",
    "    - `median`: median of the points in the region on the split dimension\n",
    "    - `middle`: middle of the region on the split dimension\n",
    "    - `best_interest_diff`: value that maximize the difference of progress in the 2 sub-regions (SAGG-RIAC)\n",
    "* `progress_win_size`: Number of last points taken into account for progress computation (should be < `max_points_per_region`)\n",
    "* `progress_measure` How to compute progress: \n",
    "    - `abs_deriv_cov`: Approach from the discrete progress interest model \n",
    "    - `abs_deriv`: absolute difference between first and last points in the window\n",
    "    - `abs_deriv_smooth`: absolute difference between first and last half of the window\n",
    "* `sampling_mode`: How to sample a point in the tree: \n",
    "    - `dict(multiscale=bool, volume=bool, mode='greedy'|'random'|'epsilon_greedy'|'softmax', param=float)`\n",
    "        - `multiscale`: if we choose between all the nodes of the tree to sample a goal, leading to a multi-scale resolution (SAGG-RIAC)\n",
    "        - `volume`: if we weight the progress of nodes with their volume to choose between them (used by `random` and `softmax` sampling modes)\n",
    "        - `mode`\n",
    "            - `greedy`: Sample a point in the leaf with the max progres\n",
    "            - `random`: Sample a point in a random leaf\n",
    "            - `epsilon_greedy`: Sample a point in the leaf with the max progress with probability (1-`eps`) and a random leaf with probability (`eps`)\n",
    "                - `epsilon`\n",
    "            - `softmax`:  Sample leaves with probabilities progress*volume and a softmax exploration (with a temperature parameter)\n",
    "                - `temperature`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef1ed8-4c64-45a1-a645-3a395ddf01bd",
   "metadata": {},
   "source": [
    "#### [IAC](https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/iac07.pdf) (2007)\n",
    "\n",
    "I think the playground experiment was using some early iterations of this Tree implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4004d-34b5-406d-baa9-b9187c7f84db",
   "metadata": {},
   "source": [
    "#### [R-RIAC](http://www.pyoudeyer.com/TAMDBaranesOudeyer09.pdf) (2009)\n",
    "TODO: read paper and populate this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a724a2-8534-4fc5-a85c-05572fce937a",
   "metadata": {},
   "source": [
    "#### [SAGG_RIAC](http://www.pyoudeyer.com/ActiveGoalExploration-RAS-2013.pdf) (2013)\n",
    "TODO: read paper and populate this more\n",
    "* $card(r_i)$: The number of points in the sub-region $i$\n",
    "* $progress(r_i)$: The absolute derivative of the competence on the points of the sub-region $r_i$\n",
    "* **Competence** on goal point $g$ with observed sensori consequence $s$: $e^{-power\\times||g-s||}$ with $power=10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e4de4-9972-4f15-b92c-d45ef548e391",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model Progress\n",
    "\n",
    "Related paper [here](https://flowers.inria.fr/FrontierscogSciJul13.pdf)\n",
    "\n",
    "TODO: Look into this more?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda4366-d670-4126-8ca9-c36d175c2ef6",
   "metadata": {},
   "source": [
    "Computes a gaussian mixture model that represents (simultaneously) the space of interest, the competence, and time (thus, a mixture in $S \\times C \\times T$ space). To sample an interesting region of $S$, the algoirthm weights the gaussian components based on their covariance between $C$ and $T$, giving positive weight to a component if the competence increases with time in that region of $S$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911fbed3-4f50-4169-b274-e415067b6187",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Context Mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a5fa2-9764-4c63-97f7-64a0470ddbde",
   "metadata": {},
   "source": [
    "_Note: I didn't dive into this much since I'm more interested in other interest models. And this seems to be captures by those?_\n",
    "\n",
    "\n",
    "Context mode can used with Random and DiscretizedProgress Interest models. If selected, will also impact Sensorimotor model. \n",
    "\n",
    "Params: \n",
    "* `mode` \n",
    "    - 'None' | 'mcs' | 'mdmsds'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3969e2-3b4c-41fd-97b6-ee419ad01d21",
   "metadata": {},
   "source": [
    "## None\n",
    "\n",
    "Seems to be the behaviour for all other Interest models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00cc182-5205-4fe4-8fd2-fe239672e9a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## mcs \n",
    "Allow the learning and control of actions that depend on the context provided by the environment (only sensory context). More details here -> learning_with_environment_context.ipynb\n",
    "\n",
    "> The sensory feedback is now the concatenation of the context  $c$ and the feedback of the arm  $s$. We thus call this mode \"mcs\".\n",
    "\n",
    "\n",
    "To draw a goal given a context, the interest model has to be 'RandomInterest' or 'DiscretizedProgress':\n",
    "\n",
    "* If RandomInterest model, will draw a random goal in the dimensions of $s$ (not $c$). i.e. will sample randomly on dimensions not in context. \n",
    "* If DiscretizedProgress model, will draw a goal in the $s$ region where the progress is maximal on points when the context was similar to $c$. i.e. Samples the region with max progress among regions that have the same context. \n",
    "\n",
    "\n",
    "MCS Specific Params:\n",
    "* `reset_iterations`: Number of iterations before the environment is reset\n",
    "* `context_n_dims`: how many dimensions the context has\n",
    "* `context_sensory_bounds`: specify the min and max bounds on those dimenions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980331a-343d-4f95-99d9-0188bc96c666",
   "metadata": {},
   "source": [
    "## mdmsds \n",
    "Define local actions that depend on the previous motor and sensory positions. Allow the learning and control of local actions that depend on a sensory and motor context. \n",
    "More details here -> learning_with_sensorimotor_context.ipynb\n",
    "\n",
    "\n",
    "MDMSDS Specific Params:\n",
    "* `choose_m`\n",
    "* `rest_positions`\n",
    "* `dm_bounds`\n",
    "* `ds_bounds`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43689c10-4fab-4d32-b032-a64a837d5a4e",
   "metadata": {},
   "source": [
    "# Things to Revisit\n",
    "\n",
    "* https://nbviewer.org/github/sebastien-forestier/ExplorationAlgorithms/blob/master/main.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12583bd-fd27-40ed-921e-bbe7d2586f7c",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- ball tree (cone tree?) with cos-sim clustering.\n",
    "- Read 'Exploration strategies in developmental robotics: a unified probabilistic framework'\n",
    "- Look into Interest Model GMM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
